# Optimizing NanoGPT

## Can optimizing the squentropy loss function and adjusting pertinent hyperparameters improve baseline NanoGPT performance?

**What is the most interesting topic covered in your domain this quarter?**
The topic I found the most interesting was seeing how the number of parameters can drastically affect the loss landscape when attempting to optimize the algorithm, especially the minima.
**Describe a potential investigation you would like to pursue for your Quarter 2 Project.**
An investigation that I would like to pursue would be understanding how to choose the most optimal loss function for any machine learning algorithm in general.
**What is a potential change youâ€™d make to the approach taken in your current Quarter 1 Project?**
One change I'd make going forward in the Quarter 2 Project compared to the first is starting off doing more research instead of just trying to get the code to run. When we first started running NanoGPT, we didn't have much of an understanding as far as what the right amount of memory to use and also the number of GPU's necessary to fit our model. Thus, looking more into the amount of resources needed to run a certain project as well as looking into what specifically we will be changing early on is something I would focus on.
**What other techniques would you be interested in using in your project?**
I think understanding how the type of data we are feeding into our model and choosing an apporpriate loss function according to the type of data we are working with is a better strategy going forward when optimizing a neural network instead of attempting very common loss functions and trying to see which one works best.
